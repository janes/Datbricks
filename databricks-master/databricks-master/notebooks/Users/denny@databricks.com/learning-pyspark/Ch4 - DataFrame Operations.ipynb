{"cells":[{"cell_type":"code","source":["# drop($col1)\n#   Drop a column within your DataFrame\n#   In the example below, we have dropped the \"level\" column from our DataFrame\n# sw.drop(\"level\").show()\ndisplay(sw.drop(\"level\"))"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":["## Learning PySpark\n### Chapter 4: DataFrames Operations\nThis notebook contains sample code from Chapter 4 of [Learning PySpark]() focusing on PySpark and DataFrame Functions"],"metadata":{}},{"cell_type":"markdown","source":["#### Generating data to be used for the various functions"],"metadata":{}},{"cell_type":"code","source":["# Generate our own JSON data \n#   This way we don't have to access the file system yet.\nstringJSONRDD = sc.parallelize((\"\"\" \n  { \"id\": \"123\",\n    \"name\": \"Katie\",\n    \"age\": 19,\n    \"eyeColor\": \"brown\",\n    \"goldDate\": \"2005-01-22\",\n    \"level\": -1\n  }\"\"\",\n   \"\"\"{\n    \"id\": \"234\",\n    \"name\": \"Michael\",\n    \"age\": 22,\n    \"eyeColor\": \"green\",\n    \"goldDate\": \"2011-11-12\",\n    \"level\": -2\n  }\"\"\", \n  \"\"\"{\n    \"id\": \"345\",\n    \"name\": \"Simone\",\n    \"age\": 23,\n    \"eyeColor\": \"blue\",\n    \"goldDate\": \"2008-06-07\",\n    \"level\": -4\n  }\"\"\")\n)\n\n\n# Generate our own JSON data \n#   This way we don't have to access the file system yet.\nstringLevelRDD = sc.parallelize((\"\"\" \n  { \n    \"level\": -1,\n    \"levelName\": \"Gold\"\n  }\"\"\",\n\"\"\"{\n    \"level\": -2,\n    \"levelName\": \"Silver\"\n  }\"\"\",\n\"\"\"{\n    \"level\": -3,\n    \"levelName\": \"Bronze\"\n  }\"\"\")\n)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Create DataFrames\nsw = spark.read.json(stringJSONRDD)\nlv = spark.read.json(stringLevelRDD)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Include pyspark.sql.functions\nfrom pyspark.sql.functions import *"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["#### Display Data\nTake a quick view of the DataFrames"],"metadata":{}},{"cell_type":"code","source":["display(sw)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["display(lv)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["### Display Operations"],"metadata":{}},{"cell_type":"code","source":["# collect()\n#  Display the data in your DataFrame as a List of row objects\nsw.collect()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# columns()\n#   Returns the columns within your DataFrame as a List\nsw.columns"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# head(n=None)\n#   Returns the first n rows.\nsw.head(2)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Show()\n#   Display the data in your DataFrame in Tabular format by using the .show(<n>); by default n = 10.\nsw.show()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### Query Operations"],"metadata":{}},{"cell_type":"code","source":["# filter(condition)\n#    Filters rows using the given condition.\nsw.filter(\"age > 20\").show()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# first()\n#   Returns the first row as a Row.\nsw.first()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# isLocal\n\n#sw.isLocal()\n"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# where\n#    Alias for filter()\nsw.where(\"age <= 20\").show()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["### Join Operations"],"metadata":{}},{"cell_type":"code","source":["# Join [Inner (default)] swimmers (sw) and level (lv) DataFrames\ndisplay(sw.join(lv, sw.level == lv.level, \"inner\") \\\n  .select(sw.name, sw.eyeColor, lv.levelName))"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# Join [Left Outer] swimmers (sw) and level (lv) DataFrames\ndisplay(sw.join(lv, sw.level == lv.level, \"left_outer\") \\\n  .select(sw.name, sw.eyeColor, lv.levelName))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# Join [Outer] swimmers (sw) and level (lv) DataFrames\ndisplay(sw.join(lv, sw.level == lv.level, \"outer\") \\\n  .select(sw.name, sw.eyeColor, lv.levelName))"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# Join [Right Outer] swimmers (sw) and level (lv) DataFrames\ndisplay(sw.join(lv, sw.level == lv.level, \"right_outer\") \\\n  .select(sw.name, sw.eyeColor, lv.levelName))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["# Join [Left Semi] swimmers (sw) and level (lv) DataFrames\ndisplay(sw.join(lv, sw.level == lv.level, \"leftsemi\") \\\n  .select(sw.name, sw.eyeColor, sw.level))"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# Intersect\n#    Return a new DataFrame containing rows only in both this frame and another DataFrame. "],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# Create the sw2 DataFrame which is missing a row \nsw2 = sw.filter(\"age >= 22\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# Show the intersection of the sw and sw2 DataFrames\nsw.intersect(sw2).show()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["### Aggregate and Grouping Operations"],"metadata":{}},{"cell_type":"code","source":["# agg\n#   shorthand for df.groupBy.agg()\nsw.agg({\"age\":\"max\"}).show()"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# groupBy\n#    Group By statement\nsw.groupBy('name').agg({'age': 'max'}).show()"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["###Cache and Plan Operations"],"metadata":{}},{"cell_type":"code","source":["# Cache the data for faster query operations\n#   Persists with the default storage level (MEMORY_ONLY).\nsw.cache()"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# explain(extended=False)\n#   Prints the (logical and physical) plans to the console for debugging purpose.\n#   `extended=True` will print out both logial and physical plans \n\n# Create DataFrame\ndf = sw.join(lv, sw.level == lv.level, \"left_outer\").select(sw.name, sw.eyeColor, lv.levelName)\n\n# Explain the plan\ndf.explain(extended=True)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["### Column Operations"],"metadata":{}},{"cell_type":"code","source":["# alias(\"$name\")\n#    Provides an alias to a column for your DataFrame\nsw.select(\n  sw.level,\n  abs(sw.level).alias('abs_level')\n).show()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["# drop($col1)\n#   Drop a column within your DataFrame\n#   In the example below, we have dropped the \"level\" column from our DataFrame\n# sw.drop(\"level\").show()\ndisplay(sw.drop(\"level\"))"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["### Partition Operations"],"metadata":{}},{"cell_type":"code","source":["# Get the number of partitions \n#  Note this is a RDD operation\nsw.rdd.getNumPartitions()"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["# Coalesce\n#  Merge partitions together df.coalesce(n) \nsw.coalesce(2).rdd.getNumPartitions()"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["# CreateOrReplaceTempView(\"$name$\")\n#   Create or replace a temporary view (will be available for the life of the `Spark Session`)\nsw.createOrReplaceTempView(\"swimmers\")"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["# createTempView(\"$name$\")\n#   Creates a temporary view (will be available for the life of the `Spark Session`)\n#   Will receive exception below if the temporary table already exists\n#sw.createTempView(\"swimmers\")"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["# dropTempTable(\"$name$\")\n#   Drop Temporary Table / view; note that you are using the `sqlContext` or `spark` instead of the DataFrame\nsqlContext.dropTempTable(\"swimmers\")"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["# distinct()\n#   Returns a DataFrame of distinct rows from your DataFrame\nsw.distinct()"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["# dropDuplicates\n#   Drop duplicates from your DataFrame based on the columns specified (in a List)\n#   If no columns are specified, then duplicate rows are dropped based on all columns\nsw.dropDuplicates(['name', 'eyeColor'])"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["# drop_duplicates\n#   Alias for dropDuplicates\nsw.drop_duplicates(['name', 'eyeColor'])"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["### Null Operations"],"metadata":{}},{"cell_type":"code","source":["# dropna(how='any', thresh=None, subset=None)\n#   Returns a new DataFrame omitting rows with null values. \n\n# First create a new DF with null values\ndf = sw.join(lv, sw.level == lv.level, \"outer\").select(sw.name, sw.eyeColor, lv.levelName)\n\n# Now drop null rows\ndf.dropna().show()"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["# fillna(value, subset=None)\n#   Replace null values, alias for na.fill(). DataFrame.fillna() and DataFrameNaFunctions.fill() are aliases of each other.\n\n# Create table using outer join with nulls\ndf = sw.join(lv, sw.level == lv.level, \"outer\").select(sw.name, sw.eyeColor, lv.levelName)\n\n# replace NULLs for the `levelName` columns\n#df.na.fill({'levelName': 'unknown'}).show()\n\n# replace NULLs for all columns\ndf.fillna('unknown').show()"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":51}],"metadata":{"name":"Ch4 - DataFrame Operations","notebookId":1653131907652720},"nbformat":4,"nbformat_minor":0}
