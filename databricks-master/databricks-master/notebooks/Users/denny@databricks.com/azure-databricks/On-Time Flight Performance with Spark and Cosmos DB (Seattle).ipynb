{"cells":[{"cell_type":"markdown","source":["# On-Time Flight Performance with Spark and Cosmos DB (Seattle)\n\n\n## On-Time Flight Performance Background\nThis notebook provides an analysis of On-Time Flight Performance and Departure Delays data using GraphFrames for Apache Spark.  \n* Original blog post: [On-Time Flight Performance with GraphFrames with Apache Spark Blog Post](https://databricks.com/blog/2016/03/16/on-time-flight-performance-with-graphframes-for-apache-spark.html)\n* Original Notebook: [On-Time Flight Performance with GraphFrames with Apache Spark Notebook](http://cdn2.hubspot.net/hubfs/438089/notebooks/Samples/Miscellaneous/On-Time_Flight_Performance.html)\n\n<img style=\"align: center;\" src=\"https://github.com/dennyglee/databricks/blob/master/images/airports-d3-m.gif?raw=true\">\n\nSource Data: \n* [OpenFlights: Airport, airline and route data](http://openflights.org/data.html)\n* [United States Department of Transportation: Bureau of Transportation Statistics (TranStats)](http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time)\n * Note, the data used here was extracted from the US DOT:BTS between 1/1/2014 and 3/31/2014*\n\nReferences:\n* [GraphFrames User Guide](http://graphframes.github.io/user-guide.html)\n* [GraphFrames: DataFrame-based Graphs (GitHub)](https://github.com/graphframes/graphframes)\n* [D3 Airports Example](http://mbostock.github.io/d3/talk/20111116/airports.html)\n\n## Spark to Cosmos DB  Connector\nConnecting Apache Spark to Azure Cosmos DB accelerates your ability to solve your fast moving Data Sciences problems where your data can be quickly persisted and retrieved using Azure Cosmos DB's DocumentDB API.  With the Spark to Cosmos DB conector, you can more easily solve scenarios including (but not limited to) blazing fast IoT scenarios, update-able columns when performing analytics, push-down predicate filtering, and performing advanced analytics to data sciences against your fast changing data against a geo-replicated managed document store with guaranteed SLAs for consistency, availability, low latency, and throughput.   \n\nThe Spark to Cosmos DB connector utilizes the [Azure DocumentDB Java SDK](https://github.com/Azure/azure-documentdb-java) will utilize the following flow:\n\n<img style=\"align: center;\" src=\"https://raw.githubusercontent.com/dennyglee/notebooks/master/images/Azure-DocumentDB-Spark_Connector_600x266.png\">\n\n\n\nThe data flow is as follows:\n\n1. Connection is made from Spark master node to Cosmos DB gateway node to obtain the partition map. Note, user only specifies Spark and Cosmos DB connections, the fact that it connects to the respective master and gateway nodes is transparent to the user.\n2. This information is provided back to the Spark master node. At this point, we should be able to parse the query to determine which partitions (and their locations) within Cosmos DB we need to access.\n3. This information is transmitted to the Spark worker nodes ...\n4. Thus allowing the Spark worker nodes to connect directly to the Cosmos DB partitions directly to extract the data that is needed and bring the data back to the Spark partitions within the Spark worker nodes."],"metadata":{}},{"cell_type":"code","source":["# Connection\nflightsConfig = {\n\"Endpoint\" : \"https://pass-cosmosdb.documents.azure.com:443/\",\n\"Masterkey\" : \"RMZXBrmg60L6tlwx52stSzt4r97WkZ2BXrI7rjuhRvZ5hiUcjiH6zmUoWeHLEqPhtv6mQE3gi8tqFOxfn97kMQ==\",\n\"Database\" : \"flights\",\n\"preferredRegions\" : \"West US; Central US\",\n\"Collection\" : \"departuredelays\", \n\"SamplingRatio\" : \"1.0\",\n\"schema_samplesize\" : \"1000\",\n\"query_pagesize\" : \"200000\",\n\"query_custom\" : \"SELECT c.date, c.delay, c.distance, c.origin, c.destination FROM c\"\n}"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["flights = spark.read.format(\"com.microsoft.azure.cosmosdb.spark\").options(**flightsConfig).load()\nflights.cache()\nflights.createOrReplaceTempView(\"flights\")\nflights.count()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["### Obtain Airport Information"],"metadata":{}},{"cell_type":"code","source":["# Set File Paths\nairportsnaFilePath = \"wasb://data@doctorwhostore.blob.core.windows.net/airport-codes-na.txt\"\n\n# Obtain airports dataset\nairportsna = spark.read.csv(airportsnaFilePath, header='true', inferSchema='true', sep='\\t')\nairportsna.createOrReplaceTempView(\"airports\")"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["### Flights departing from Seattle"],"metadata":{}},{"cell_type":"code","source":["%sql\nselect count(1) from flights where origin = 'SEA'"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["### Top 10 Delayed Destinations originating from Seattle"],"metadata":{}},{"cell_type":"code","source":["%sql\nselect a.city as destination, sum(f.delay) as TotalDelays, count(1) as Trips\nfrom flights f\njoin airports a\n  on a.IATA = f.destination\nwhere f.origin = 'SEA'\nand f.delay > 0\ngroup by a.city \norder by sum(delay) desc limit 10"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["### Calculate median delays by destination cities departing from Seattle"],"metadata":{}},{"cell_type":"code","source":["%sql\nselect a.city as destination, percentile_approx(f.delay, 0.5) as median_delay\nfrom flights f\njoin airports a\n  on a.IATA = f.destination\nwhere f.origin = 'SEA'\ngroup by a.city \norder by percentile_approx(f.delay, 0.5)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["## Building up a GraphFrames\nUsing GraphFrames for Apache Spark to run degree and motif queries against Cosmos DB"],"metadata":{}},{"cell_type":"code","source":["# Build `departureDelays` DataFrame\ndepartureDelays = spark.sql(\"select cast(f.date as int) as tripid, cast(concat(concat(concat(concat(concat(concat('2014-', concat(concat(substr(cast(f.date as string), 1, 2), '-')), substr(cast(f.date as string), 3, 2)), ' '), substr(cast(f.date as string), 5, 2)), ':'), substr(cast(f.date as string), 7, 2)), ':00') as timestamp) as `localdate`, cast(f.delay as int), cast(f.distance as int), f.origin as src, f.destination as dst, o.city as city_src, d.city as city_dst, o.state as state_src, d.state as state_dst from flights f join airports o on o.iata = f.origin join airports d on d.iata = f.destination\") \n\n# Create Temporary View and cache\ndepartureDelays.createOrReplaceTempView(\"departureDelays\")\ndepartureDelays.cache()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Note, ensure you have already installed the GraphFrames spack-package\nfrom pyspark.sql.functions import *\nfrom graphframes import *\n\n# Create Vertices (airports) and Edges (flights)\ntripVertices = airportsna.withColumnRenamed(\"IATA\", \"id\").distinct()\ntripEdges = departureDelays.select(\"tripid\", \"delay\", \"src\", \"dst\", \"city_dst\", \"state_dst\")\n\n# Cache Vertices and Edges\ntripEdges.cache()\ntripVertices.cache()\n\n# Create TripGraph\ntripGraph = GraphFrame(tripVertices, tripEdges)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### What flights departing SEA with the most significant average delays\nNote, the joins are there to see the city name instead of the IATA codes.  The `rank()` code is there to help order the data correctly when viewed in Jupyter notebooks."],"metadata":{}},{"cell_type":"code","source":["flightDelays = tripGraph.edges.filter(\"src = 'SEA' and delay > 0\").groupBy(\"src\", \"dst\").avg(\"delay\").sort(desc(\"avg(delay)\"))\nflightDelays.createOrReplaceTempView(\"flightDelays\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%sql\nselect a.city, `avg(delay)` as avg_delay \nfrom flightDelays f\njoin airports a\non f.dst = a.iata\norder by `avg(delay)` \ndesc limit 10"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["### Which is the most important airport (in terms of connections)\nIt would take a relatively complicated SQL statement to calculate all of the edges to a single vertex, grouped by the vertices.  Instead, we can use the graph `degree` method."],"metadata":{}},{"cell_type":"code","source":["airportConnections = tripGraph.degrees.sort(desc(\"degree\"))\nairportConnections.createOrReplaceTempView(\"airportConnections\")"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["%sql\nselect a.city, f.degree \nfrom airportConnections f \njoin airports a\n  on a.iata = f.id\norder by f.degree desc \nlimit 10\n"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["### Are there direct flights between Seattle and San Jose?"],"metadata":{}},{"cell_type":"code","source":["filteredPaths = tripGraph.bfs(\n    fromExpr = \"id = 'SEA'\",\n    toExpr = \"id = 'SJC'\",\n    maxPathLength = 1)\ndisplay(filteredPaths)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["### But are there any direct flights between San Jose and Buffalo?\n* Try maxPathLength = 1 which means one edge (i.e. one flight) between `SJC` and `BUF`, i.e. direct flight\n* Try maxPathLength = 2 which means two edges between `SJC` and `BUF`, i.e. all the different variations of flights between San Jose and Buffalo with only one stop oever in between?"],"metadata":{}},{"cell_type":"code","source":["filteredPaths = tripGraph.bfs(\n  fromExpr = \"id = 'SJC'\",\n  toExpr = \"id = 'BUF'\",\n  maxPathLength = 1)\ndisplay(filteredPaths)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["filteredPaths = tripGraph.bfs(\n  fromExpr = \"id = 'SJC'\",\n  toExpr = \"id = 'BUF'\",\n  maxPathLength = 2)\ndisplay(filteredPaths)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["### In that case, what is the most common transfer point between San Jose and Buffalo?"],"metadata":{}},{"cell_type":"code","source":["display(filteredPaths.groupBy(\"v1.id\", \"v1.City\").count().orderBy(desc(\"count\")).limit(10))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["## Predicting Flight Delays\nExtending upon analysis we have done up to this point, can we also predict if a flight will be delayed, on-time, or early based on the available data.\n\n### Prepare the Dataset\nThe first thing we will do is to cleanse the data and apply some labels to our information (e.g. early, on-time, delayed).  As well, we will want to remove any rows with NULL values."],"metadata":{}},{"cell_type":"code","source":["# This contains a generated mapping between tripid and airline\n#   You can get the file at https://github.com/dennyglee/databricks/blob/master/misc/trip_airline_map.csv\n#   For this example, the trip_airline_map.csv file has been pushed to in my mounted bucket.\ntripAirlineMapFilePath = \"wasb://data@doctorwhostore.blob.core.windows.net/trip_airline_map.csv\"\ntripAirlineMap = spark.read.csv(tripAirlineMapFilePath, sep=\",\", header=True)\ntripAirlineMap.createOrReplaceTempView(\"tripAirlineMap\")"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# Prep dataset\n# Including only Seattle and Las Vegas for this demo\nflightML = spark.sql(\"select cast(distance as double) as distance, src as origin, state_src as origin_state, dst as destination, state_dst as destination_state, concat(concat(concat(cast(tripid as string), src), dst), cast((delay + 2000) as string)) as trip_identifier, case when delay <= 0 then 'on-time' else 'delayed' end as flight_status from departureDelays where src IN ('LAS', 'SEA')\")\nflightML = flightML.dropna().dropDuplicates()\nflightML.createOrReplaceTempView(\"flightML\")\n"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# Join flights and airline information\ndataset = spark.sql(\"select f.distance, f.origin, f.origin_state, f.destination, f.destination_state, f.trip_identifier, f.flight_status, m.airline from flightML f join tripAirlineMap m on m.trip_identifier = f.trip_identifier\")\ndataset = dataset.dropDuplicates()\n#dataset = flightML\ncols = dataset.columns"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["dataset.printSchema()"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["### Building ML Pipeline\nBefore we can run our various models against this data, we will first need to vectorize our data via One-Hot Encorder (for category data), String Indexer (create an index based on our labelled values), and Vector Assembler."],"metadata":{}},{"cell_type":"code","source":["# One-Hot Encoding\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n\ncategoricalColumns = [\"origin\", \"origin_state\", \"destination\", \"destination_state\", \"trip_identifier\", \"airline\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n  # Category Indexing with StringIndexer\n  stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n  \n  # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n  encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n  \n  # Add stages.  These are not run here, but will run all at once later on.\n  stages += [stringIndexer, encoder]\n\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol = \"flight_status\", outputCol = \"label\")\nstages += [label_stringIdx]\n\n# Transform all features into a vector using VectorAssembler\nnumericCols = [\"distance\"]\nassemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# Create a Pipeline.\npipeline = Pipeline(stages=stages)\n# Run the feature transformations.\n#  - fit() computes feature statistics as needed.\n#  - transform() actually transforms the features.\npipelineModel = pipeline.fit(dataset)\ndataset = pipelineModel.transform(dataset)\n\n# Keep relevant columns\nselectedcols = [\"label\", \"features\"] + cols\ndataset = dataset.select(selectedcols)\ndisplay(dataset)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["### Randomly split data into training and test datasets\n* Set the seed for reproducibility"],"metadata":{}},{"cell_type":"code","source":["(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["## Logistic Regression\nLet's try using logistic regression to see if we can accurately predict if a flight will be delayed.\n* First, we will train the data using Logistic Regression\n* Next we will run that model against the testData"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n\n# Train model with Training Data\nlrModel = lr.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["# Make predictions on test data using the transform() method.\n# LogisticRegression.transform() will only use the 'features' column.\npredictions = lrModel.transform(testData)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["### View LR Model's predictions\n* Recall, label is the actual test value, prediction is the predicted value\n * where 0 - on-time, 1 - delayed"],"metadata":{}},{"cell_type":"code","source":["selected = predictions.select(\"label\", \"prediction\", \"probability\", \"flight_status\", \"destination\", \"destination_state\").where(\"destination = 'SEA'\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["#### Evaluate our model\nLet's use the `BinaryClassificationEvaluator` to determine the precision of our model."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["predictions.createOrReplaceTempView(\"predictions\")"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["%sql\nselect * from predictions limit 10"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["%sql\nselect confusion, count(1)\nfrom (\nselect \ncase\n  when label = 0 and prediction = 0 then 'True Positives (On-Time, Predicted: On-Time)'\n  when label = 0 and prediction = 1 then 'False Negatives (On-Time, Predicted: Delayed)'\n  when label = 1 and prediction = 0 then 'False Postitives (Delayed, Preidcted: On-Time)'\n  when label = 1 and prediction = 1 then 'True Negatives (Delayed, Predicted: Delayed)'\nend as confusion\nfrom predictions\n) a\ngroup by confusion\n"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["### Plot decision boundary of the logistic regression model"],"metadata":{}},{"cell_type":"code","source":["# Take a sample of 200 to plot decision boundary of logistic regression\npredictions_sample = predictions.sample(False, 0.1, seed=100).limit(200)\npredictions_sample.createOrReplaceTempView(\"predictions_sample\")\n\n# Build schema for data\nfrom pyspark.sql.types import *\nschema = StructType([\n        StructField('X1', FloatType(), True), \n        StructField('X2', StringType(), True),\n        StructField('y', IntegerType(), True)])\n\n# Extract data\nps = predictions_sample.rdd.map(lambda p: (float(p.rawPrediction[0]), p.airline, p.label) )\n\n# Convert to DataFrame\npsDF = ps.toDF([\"X1\", \"X2\", \"y\"], schema)\npsDF.createOrReplaceTempView(\"psDF\")"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["import numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\nimport matplotlib.pyplot as plt\n\n#X, y = make_classification(200, 2, 2, 0, weights=[.5, .5], random_state=15)\nX = np.asarray(pyX)\nz = np.asarray(pyY, dtype=np.int)\ny = z.ravel() \n\nclf = LogisticRegression().fit(X[:100], y[:100])\n\nxx, yy = np.mgrid[-5:5:.01, -5:5:.01]\ngrid = np.c_[xx.ravel(), yy.ravel()]\nprobs = clf.predict_proba(grid)[:, 1].reshape(xx.shape)\n\nf, ax = plt.subplots(figsize=(8, 6))\ncontour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\",\n                      vmin=0, vmax=1)\nax_c = f.colorbar(contour)\nax_c.set_label(\"$P(y = 1)$\")\nax_c.set_ticks([0, .25, .5, .75, 1])\n\nax.scatter(X[100:,0], X[100:, 1], c=y[100:], s=50,\n           cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n           edgecolor=\"white\", linewidth=1)\n\nax.set(aspect=\"equal\",\n       xlim=(-5, 5), ylim=(-5, 5),\n       xlabel=\"$X_1$\", ylabel=\"$X_2$\")"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":51}],"metadata":{"name":"On-Time Flight Performance with Spark and Cosmos DB (Seattle)","notebookId":3801055605060586},"nbformat":4,"nbformat_minor":0}
